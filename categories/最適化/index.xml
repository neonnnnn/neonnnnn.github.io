<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>最適化 on 積んでるゲームのサントラを聴く</title><link>https://neonnnnn.github.io/categories/%E6%9C%80%E9%81%A9%E5%8C%96/</link><description>Recent content in 最適化 on 積んでるゲームのサントラを聴く</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><copyright>Kyohei Atarashi</copyright><lastBuildDate>Mon, 25 Oct 2021 01:56:30 +0900</lastBuildDate><atom:link href="https://neonnnnn.github.io/categories/%E6%9C%80%E9%81%A9%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>0階/1階最適化アルゴリズムの反復数の下界</title><link>https://neonnnnn.github.io/posts/nesterov-book-lower-bound/</link><pubDate>Mon, 25 Oct 2021 01:56:30 +0900</pubDate><guid>https://neonnnnn.github.io/posts/nesterov-book-lower-bound/</guid><description>はじめに 様々な最適化問題に対して、それを解くための様々な最適化アルゴリズムが提案されています。 最適化問題の解きやすさ・最適化アルゴリズムの良し悪しの議論の際に、収束レート、すなわち、$\varepsilon$-近似解を得るために必要な反復数のオーダーというものを考えることがあります（あるいは、結局は同じことですが、反復数に対する精度$\varepsilon$のオーダー）。 Nesterov先生の本、Lectures on Convex Optimizationに、0階/1階の最適化アルゴリズムという比較的広いクラスの最適化アルゴリズムの反復数の下界の証明があり、そういえばステートメントは見たことがあったが証明までは見たことがなかったと思い、自分の言葉でややカジュアルに整理してみます（外せる・緩めることのできる仮定などもありますが、とりあえずはそのままで）。
問題設定 問題設定 何らかの連続性を持つ関数$f:\mathbb{R}^n \to \mathbb{R}$に対して、以下の最適化問題を考えます： \begin{align} \min_{x \in C \subseteq \mathbb{R}^n } f(x). \end{align} また、「連続最適化問題を解く」は「$\varepsilon$-近似解（$\varepsilon&amp;gt;0$）を得る」こと、すなわち、 $$f(\bar{x}) - f(x^) &amp;lt; \varepsilon, \ \text{where} \ x^ = \underset{x\in C} {\mathrm{argmin}}f(x)$$ を満たす$\bar{x} \in C$を得ること、と定義します。 この$\bar{x}$を得るための反復数の下界、すなわち、最悪ケースでどの程度の反復数が必要になるのか、ということについて考えます。
考える最適化アルゴリズム 様々な最適化アルゴリズムが存在しますが、その多くは反復法と呼ばれる枠組みに入ります。 反復法は、ひどく一般化して書くと、以下のような手順で最適化を行います：
初期解$x_0$と精度$\varepsilon$が与えられる。$k=0$とする。 現在の解$x_k$でオラクルを呼ぶ。オラクルは最適化問題に関する何らかの情報を返す。情報の集合にオラクルから得られた情報を追加する。 情報の集合と、アルゴリズムが定める規則に基づいて、次の解$x_{k+1}$を生成する。 収束条件のチェックをする。$f(x_{k+1}) - f(x^*) &amp;lt; \varepsilon$ならば$\bar{x}:=x_{k+1}$として$\bar{x}$を返す。さもなければ、$k \leftarrow k+1$として、2に戻る。 「オラクルからどのような情報を得るか」「どのように更新するか」が、アルゴリズムの設計において重要になってきます（前者は、設計というよりも問題設定・前提、と言ったほうが正確な気もしますが）。 ここでは、以下の二種類のオラクルについて考えます。
0階オラクル：関数の値$f(x_k)$を返す。 1階オラクル：関数の値$f(x_k)$とその勾配$\nabla f(x_k)$を返す。 0階オラクルを用いる場合（しか用いることができない場合）は、いわゆるブラックボックス関数の最適化問題になるかと思います。 また、1階オラクルを用いる手法はいわゆる勾配ベースの方法になります。 0階オラクルを用いる最適化アルゴリズムを0階の最適化アルゴリズム、1階のオラクルを用いる最適化アルゴリズムを1階の最適化アルゴリズムと呼ぶことにします。
0階の最適化アルゴリズムの場合の下界 最適化問題に対する仮定 実行可能領域として、以下を考えます： \begin{align} B_n = \{x \in \mathbb{R}^n \mid 0 \le x^{(i)} \le 1, i=1,\ldots, n\}.</description></item></channel></rss>